# To myself, sometime in the future

*** 
Basically, I forgot where I was in this project. I haven't touched it a couple months. I remember I had discovered that I had been deceived by what looked like a well structured data source. I had built out an approach to downloading tons of data from IRS 990's. But while the data was there (at least what was there), it was largely unusable. That's because the IRS likes to switch up xml path names and re-define what one path means every other year. It's almost like they try to make it difficult to perform detailed large scale data research and data analysis...

Anyway, I was becoming increasingly annoyed with the databases I was generating. These databases would become unweildy, confusing, and were plagued with missing data that I know was out there. This was the case for every approach I took. At the least extreme the results would be a row of data for an organization but with only 60 or 70 fields with data out of some 1000 potential fields (that's very common with 990EZ filings). Notice though, I describe this as the least extreme... The more extreme case would be where the data is pulled according to the xpath but because every single path was at one point in time renamed, the number of columns ballons from some 1000 legitamite data fields to something like 5000+ fields (I'm exaggerating here. In fact there there is a nice upside to this (more on that later) but for the most part it's a nightmare).

I'd like to state that I am only an amature when it comes to data manageemnt, data cleansing, data organization, and generally anything related to data wrangling. I've only had a few formal classes while in Graduate School. My specialty is in data analysis. So it might simply be that my methodology and approach suck. I'd love it if someone could show me a better way for the following stuff.



Imagin a program that gathers up all the data for a 990 filing and adds it to a database. The database is designed such that every row is a unique IRS filing. The columns describe the datafields of the 990 forms such as `organizational name`, `number of volunteers`, `mission expenses`, `mission description`, etc. If a individual filing doesn't have information in a field, the database records that datapoint as `NaN`. A datapoint is therefor described as either having data or not. There exists a datapoint for each intersection of a row and column. If there are 8 rows and 10 columns (fields) then there is 80 datapoints. Another example: 20 rows and 10 columns equates to 200 datapoints. You get the idea. Additioanlly if a filing has a datafield that does not match any description of the existing columns (simplist test for that is looking at it's xpath) it is added as a new column. All other rows would record a `NaN` for that column. (This is essentially what my code currently does.)

While I know the pitfalls of this approach (see further down) I lack the knowledge to know if there is a better method for handling newly encountered datafields (except for mapping, see later). For the longest time I've been content with this approach. It captures the data *as is* with respect to the datafield. If this happens with a handful of datafields that are slightly different but  all contain the same type of data and can be safetly merged together, I'm fine with that little bit of extra data wrangling. 

## How *bad* could it be?

The above mindset really only works for the following near perfect situation: consider a hypothetical national nonprofit with 500 local chapters each of which must file an annual 990 form with the IRS and each form has exactly 200 datafields. Imagin we want to build a database of the past 5 years of data. That would translate to `500 chapters * 5 years` = `2500 rows` such that each row is a unique filing and each row has 200 data fields, so multiply those `2500 rows` by `200 data fields` to get `500,000 datapoints` for this hypothetical 5 year dataset. Suppose also that at some point the IRS reviewed the xml structure and decided to rewrite the xpath for, say `/IRS990/Revenues`, because it's actually a misnomer. That is, it turns out the programmers made a mistake and this variable is an aggregate of all revenues and thus should be labeled as `IRS990/TotalRevenues`. This fix is reflected in later filings which start showing `NaN`  in the `IRS990/Revenues` column and data start's showing up in the `IRS990/TotalRevenues` column. When everything is said and done, the database actually ends up with `201 data fields`.  

But that's a near perfect world where the IRS rarely change the xpaths. We don't live in that world. We live in a world where the IRS made numerous changes to xpath names between 2012 and 2013. So just don't go back further than 2013, right? Except there were organizations filing in around 2014 and 2015 using the 2012 version.... Which is just... fantastic... And then there is the whole difference between the original 990 and the 990EZ. Pretty much every tag is differnt. 

What does this all mean? Take the same hypothetical from above but now lets face reality. Which means a lot of missaligned data types. The code goes and grabs the 2014 filing for our first local chapter. It returned some 258 unique data fields. Nothing bad yet. Then it goes and grabs the next annual filing for that same chapter and this time the chapter has 20 additional fields. Ok, so we know either the local chapter had some addiitonal information to report, or the IRS changed something. Still no big deal. And then it goes looking for year after that and can't find anything because the chapter is 3 years behind on filing... So the code moves on to the next local chapter and retrieves it's 2014 filing. Only this local chapter is small enough that it files a 990EZ. The code looks at that and says "these are unique fields". It adds 100 new fields. Ok, so now the number of columns has increased to 378 unique data fields. Keep in mind that in the last row, there are only 100 datapoints out of 378 fields. Additionally the first two rows wouldn't have any data for the 990EZ fields (because they are traditional 990s) so that data will also be missing. Now, add the next year from the small chapter, and assume no new fields. So far our hypotheical code has only retrieved four 990 forms, but what does our dataframe look like.

> 1st 990: start with `258` datapoints in a  `1 x 258` dataframe.

> 2nd 990: add `278` datapoints, so `536` datapoints in a `2 x 278` dataframe (that's 536 out of 556, so, not bad.)

> 3rd 990: add `100` datapoints, so `636` datapoints but now dataframe is `3 x 378`. (that's 636 out of 1134, so, 44% of our dataframe is `NaN`)

> 4th 990: add `100` datapoints, so `736` datapoints but now dataframe is `4 x 378`. (that's 736 out of 1512, so 51% of the dataframe is `NaN`)

Our dataframe has balloned in size. Now repeat this whole process some 500 more times and see what you end up with. You will end up with a huge database with tons of missing data. Now here's the kicker. The majority of those 'unique' fields actually represent data that easily could have been placed in another field (especially the fields generated because of the 990EZ). For instance, the IRS made the name `.TotalRevenueAmt` for the 990EZ to describe, well total revenue, while at the same time making `CYTotalRevenue` for the normal 990 to describe the EXACT SAME THING. So the *simple* and *easy* method to fix this is to CREATE A FREAKING MAPPING FOR ALL 7000+ VARIABLES! Oh what fun.

So, that's where I'm at with this. I'm turning to xml-990-Reader to help me. I'll see where that leads me...
