{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To myself, sometime in the future\n",
    "Basically forgot where I was with this project. I remember I had discovered that I had been deceived by what looked like a well structured data source. I had built out an approach to downloading tons of data from IRS 990's. But while the data was there (at least what was there), it was largely unusable. Because the IRS likes to switch up xml path names and re-define what one path means every other year. It's almost like they try to make it difficult to perform detailed large scale data research and data analysis.\n",
    "\n",
    "Anyway, the problem is that the database would become unweildy, confusing, and full of missing data. This was the case for every approach I took. At the least extreme the results would be a row of data for an organization but with only 60 or 70 filled fields out some 1000 potential fields (common with 990EZ filings). Notice, I describe this as the least extreme... The more extreme case would be where the data is pulled according to the xpath but because every single path has at one point been renamed, the number of columns ballons from some 1000 legitamite data fields to something 5000 + fields. Suprisingly there there is an upside to that (more on that later) but for the most part it's a nightmare. \n",
    "\n",
    "## How bad can it be?\n",
    "\n",
    "Consider a hypothetical program that gathers up all the data for a filing and adds it to a database. The database is designed such that every row is a unique IRS filing. The columns describe the datafields of the 990 forms. Datafields such as `organizational name`, `number of volunteers`, `mission expenses`, `mission description`, etc. If a individual filing doesn't have information in a field, the database records that datapoint as `NaN`. A datapoint is describes as either having data or not.\n",
    "\n",
    "Consider a hypothetical national nonprofit with 500 local chapters each of which files an annual form and each form has roughly 200 data fields. Imagin we want to build a database of the past 5 years of data. That would translate to `500 chapters * 5 years` = `2500 rows` such that each row is a unique filing and each row has 200 data fields so multiply those `2500 rows` by `200 data fields` to get `500,000 datapoints` in this hypothetical dataset. \n",
    "\n",
    "But that's a perfect world where the IRS never changes the xpaths (and subsequently the fields). We don't live in that world. Take the same hypothetical from above. \n",
    "\n",
    "Currently I'm considering XML-990-Gatherer's 'standardizing' feature. What do I mean here? Well, XML-990-Gather can convert each 990's XML file to `TXT`, `CSV`, or `JSON`. Which sounds like nothing special. But, according to the module author, the JSON output gets special treatment in that it is 'standardized'. For instance the code knows that the xpath `Return.ReturnHeader.TaxPeriodEnd` from a version built in 2014 contains the same type of data as another version where the xpath is instead named `Return.ReturnHeader.TxPrdEnd`. The code says, \"these path names are the same thing and the standardized version will be `Return.ReturnHeader.TaxPeriodEnd`.\"  By the way, this isn't a real example, but you get the point. It just knows both xpaths are designed to describe the same thing: the tax period end (duh). \n",
    "\n",
    "But if you output the XML to a CSV or to TXT you get no special treatment. Both provide the xpaths *as is*, meaning when it comes time to build the database you end up with more columns, a lot of empty space, and you will need to do **a lot** (and when I say **a lot** I mean an absolute truck load) of additioanl data-wrangling. And it's pretty much a case by case situation (almost). The one upside is that such a database would exactly describe the data as it was pulled. No data for that organization. But if you were say, pulling data to quickly turn around and send to people for analysis and those people aren't exactly gifted at wrangling the data before analysis, you will end up with a lot of emails complaining that the data is incomplete.\n",
    "\n",
    "> \"Yes I know that organization X is missing data for board member #11, but it's because that organization only has 10 board members!\" \n",
    "\n",
    "So, you take an afternoon to wrangle the data for them, and that afternoon turns into 2 because you've got to do your due diligence making sure you are merging relavent data and after a week of reading 990's you send the completed dataset to them and then they ask what happened to all that other data... Not that I've experienced this or anything... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}